{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 01:27:16.348121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-13 01:27:17.008758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142505\n"
     ]
    }
   ],
   "source": [
    "txt_file = 'data/eng_elec_text.txt'\n",
    "raw_corpus = []\n",
    "\n",
    "with open(txt_file, 'r', encoding = 'utf-8') as f:\n",
    "    raw = f.read().splitlines()\n",
    "    raw_corpus.extend(raw)\n",
    "print(len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186874\n"
     ]
    }
   ],
   "source": [
    "#txt_file = 'data/eng_folk_text.txt'\n",
    "#txt_file = 'data/eng_hiphop_text.txt'\n",
    "#txt_file = 'data/eng_pop_text.txt'\n",
    "#txt_file = 'data/eng_rnb_text.txt'\n",
    "txt_file = 'data/eng_rock_text.txt'\n",
    "with open(txt_file, 'r', encoding = 'utf-8') as f:\n",
    "    raw = f.read().splitlines()\n",
    "    raw_corpus.extend(raw)\n",
    "print(len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()                         #소문자 변경 후 양쪽 공백 제거\n",
    "    sentence = re.sub(r\"([?.!,¿]).,\", r\" \\1 \", sentence)          #특수문자 양쪽에 공백 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                 #여러 개의 공백이 붙어있으면 하나의 공백으로\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?!¿.,]+\", \" \", sentence)   #영어, 알파벳, ?, !, ¿ 제외 모두 공백으로\n",
    "    sentence = sentence.strip()                                 #양쪽 공백 제거\n",
    "    sentence = re.sub(r\"\\(.\\)\", \" \", sentence)                  #괄호 제거\n",
    "    sentence = '<start> ' + sentence + ' <end>'                 #start, end 추가\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132836\n",
      "['<start> ooh i can t pretend <end>', '<start> like you didn t bring <end>', '<start> my tempo up again <end>', '<start> tempo up again <end>', '<start> my head s in a spin <end>', '<start> you send my body to a place <end>', '<start> it s never been <end>', '<start> baby won t you let me <end>', '<start> keep you up all night <end>', '<start> let the morning come closer <end>']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if sentence[-1] == ':':\n",
    "        continue\n",
    "    if len(sentence)>150:\n",
    "        continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "print(len(corpus))\n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 25000, filters = ' ', oov_token = '<unk>')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    print('토크나이저: ', tokenizer, '\\n', tensor)\n",
    "    \n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저:  <keras.src.legacy.preprocessing.text.Tokenizer object at 0x7f958958a3d0> \n",
      " [[   2   66    4 ...    0    0    0]\n",
      " [   2   23    5 ...    0    0    0]\n",
      " [   2   13 3247 ...    0    0    0]\n",
      " ...\n",
      " [   2   13 5559 ...    0    0    0]\n",
      " [   2   13   13 ...    0    0    0]\n",
      " [   2 5559    3 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132836, 36)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : you\n",
      "6 : the\n",
      "7 : it\n",
      "8 : to\n",
      "9 : me\n",
      "10 : t\n",
      "11 : and\n",
      "12 : a\n",
      "13 : my\n",
      "14 : s\n",
      "15 : m\n",
      "16 : that\n",
      "17 : in\n",
      "18 : we\n",
      "19 : on\n",
      "20 : your\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, ':', tokenizer.index_word[idx])\n",
    "    if idx>=20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서 길이:  (1132836, 36)\n",
      "소스문장 길이:  35\n",
      "타겟문장 길이:  35\n"
     ]
    }
   ],
   "source": [
    "#소스 문장과 타겟 문장으로 1차 분리\n",
    "#tensor에서 마지막 토큰을 잘라내어 소스 문장을, 첫번째 start를 잘라내어 타겟 문장을 생성\n",
    "#LSTM에서 many-to-many의 답을 얻을 것이므로 위와 같이 구성 <- ????\n",
    "#이 과정을 통해 src_input과 tgt_input 길이 동일\n",
    "src_input = tensor[:, :-1]      #start+sentence+end+padding n-1개\n",
    "tgt_input = tensor[:, 1:]       #sentence+end+padding n개\n",
    "print('텐서 길이: ', tensor.shape)\n",
    "print('소스문장 길이: ', len(src_input[0]))\n",
    "print('타겟문장 길이: ', len(tgt_input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train 길이:  (906268, 35)\n",
      "Target Train 길이:  (906268, 35)\n",
      "Source Test 길이:  (226568, 35)\n",
      "Target Test 길이:  (226568, 35)\n"
     ]
    }
   ],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 1234)\n",
    "print('Source Train 길이: ', enc_train.shape)\n",
    "print('Target Train 길이: ', dec_train.shape)\n",
    "print('Source Test 길이: ', enc_val.shape)\n",
    "print('Target Test 길이: ', dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 01:28:03.274497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=(TensorSpec(shape=(256, 35), dtype=tf.int32, name=None), TensorSpec(shape=(256, 35), dtype=tf.int32, name=None))>\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(256, 35), dtype=tf.int32, name=None), TensorSpec(shape=(256, 35), dtype=tf.int32, name=None))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 01:28:03.313166: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:03.313336: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:03.314245: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:03.314378: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:03.314483: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:04.341797: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:04.341944: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:04.342051: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:04.342138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12702 MB memory:  -> device: 0, name: NVIDIA A16-16Q, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epochs = len(src_input) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset_train = dataset_train.shuffle(BUFFER_SIZE)\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "print(dataset_train)\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)      #vocab_size로 입력되어서 그걸 embedding_size 만큼으로 표현할 것\n",
    "        self.rnn_1 = tf.keras.layers.SimpleRNN(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.SimpleRNN(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)     #vocab_size로 줄여야 다음에 무슨 단어를 낼지에 대한 각 클래스 별 수치가 출력\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextGenerator name=text_generator, built=False>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 256    #word embedding 차원 수. 즉, 단어가 추상적으로 표현되는 크기 -> dataset의 shape과 같아야 하는거 아닌지???\n",
    "hidden_size = 1024      #hidden state의 차원 수\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 35, 25001), dtype=float32, numpy=\n",
       "array([[[-3.03771417e-03, -8.19578301e-03,  1.54584798e-03, ...,\n",
       "          6.85258943e-04, -1.03847422e-02, -5.12672309e-03],\n",
       "        [-8.12245067e-03,  3.23392916e-04, -1.50562450e-02, ...,\n",
       "          1.49418442e-02,  6.01206254e-03, -1.42006222e-02],\n",
       "        [ 1.32533023e-05,  2.06482206e-02,  9.89287160e-03, ...,\n",
       "          3.37677589e-03,  8.69248062e-03, -6.55518845e-03],\n",
       "        ...,\n",
       "        [-2.95723788e-02, -4.89470549e-02,  7.39559298e-04, ...,\n",
       "         -1.36053236e-02, -1.44807659e-02,  2.75651105e-02],\n",
       "        [-1.30156158e-02,  1.53294103e-02, -7.71084949e-02, ...,\n",
       "         -5.90176880e-02, -4.26432397e-03,  1.98519621e-02],\n",
       "        [ 8.43936577e-03,  1.08757224e-02, -3.63128074e-02, ...,\n",
       "          3.31647918e-02,  1.21574029e-02, -4.41858470e-02]],\n",
       "\n",
       "       [[-3.03771417e-03, -8.19578301e-03,  1.54584798e-03, ...,\n",
       "          6.85258943e-04, -1.03847422e-02, -5.12672309e-03],\n",
       "        [-1.14499517e-02, -4.24445001e-03, -4.00535529e-03, ...,\n",
       "          8.75088666e-03,  2.19989126e-03, -1.94447357e-02],\n",
       "        [-2.08819602e-02, -5.13828883e-04, -2.98414822e-03, ...,\n",
       "         -1.96517794e-06,  3.21160117e-03, -9.38512478e-03],\n",
       "        ...,\n",
       "        [-4.61182073e-02,  8.56168289e-03, -1.58114284e-02, ...,\n",
       "          1.11754090e-02,  3.06369830e-02,  1.81709731e-03],\n",
       "        [-7.67125282e-04, -3.34795788e-02, -2.10191254e-02, ...,\n",
       "         -2.74692252e-02,  1.18298922e-02, -8.16436857e-03],\n",
       "        [-1.07503794e-01, -3.08956057e-02, -6.72896653e-02, ...,\n",
       "         -2.74578575e-02,  2.40491349e-02, -6.70542642e-02]],\n",
       "\n",
       "       [[-3.03771417e-03, -8.19578301e-03,  1.54584798e-03, ...,\n",
       "          6.85258943e-04, -1.03847422e-02, -5.12672309e-03],\n",
       "        [-1.04100751e-02,  4.18287190e-03, -1.06865577e-02, ...,\n",
       "          6.69956114e-03,  7.51770334e-03, -1.44811040e-02],\n",
       "        [-4.81385551e-03, -8.31118971e-03,  1.57721769e-02, ...,\n",
       "         -7.19537539e-03, -1.21335955e-02, -8.15078430e-03],\n",
       "        ...,\n",
       "        [ 4.12742794e-02,  2.51356978e-02, -1.08969128e-02, ...,\n",
       "         -4.67225462e-02,  3.57944183e-02,  1.79771893e-02],\n",
       "        [-1.62216695e-03, -3.11087957e-03, -6.95044128e-03, ...,\n",
       "         -6.55813962e-02,  6.03902638e-02,  8.24020710e-03],\n",
       "        [-1.16030343e-01, -5.67535721e-02,  3.66419889e-02, ...,\n",
       "         -6.05390482e-02,  1.00678401e-02, -1.54697979e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.03771417e-03, -8.19578301e-03,  1.54584798e-03, ...,\n",
       "          6.85258943e-04, -1.03847422e-02, -5.12672309e-03],\n",
       "        [-2.73543876e-03,  9.74722323e-04,  3.05125793e-03, ...,\n",
       "          4.88176756e-03,  7.54812732e-03, -1.26530072e-02],\n",
       "        [ 1.30537571e-02,  5.33104269e-03,  1.48360487e-02, ...,\n",
       "         -4.66757454e-03, -1.96491629e-02, -1.78989023e-02],\n",
       "        ...,\n",
       "        [-6.86557218e-02, -1.04811862e-02,  6.66942541e-03, ...,\n",
       "         -2.34074425e-04,  6.81644306e-02,  9.51373368e-05],\n",
       "        [-8.67502578e-03,  1.04176747e-02, -4.93874364e-02, ...,\n",
       "         -7.02034310e-02, -5.93280140e-03,  3.38584855e-02],\n",
       "        [-4.55894619e-02, -3.33479159e-02, -2.49369573e-02, ...,\n",
       "         -2.62302682e-02, -2.42794678e-02, -4.12380770e-02]],\n",
       "\n",
       "       [[-3.03771417e-03, -8.19578301e-03,  1.54584798e-03, ...,\n",
       "          6.85258943e-04, -1.03847422e-02, -5.12672309e-03],\n",
       "        [-1.83103476e-02,  3.52610298e-03, -2.96536298e-03, ...,\n",
       "          9.43831541e-03,  4.59551066e-03, -9.62250121e-03],\n",
       "        [-8.71792715e-03, -1.36287818e-02,  5.98312588e-03, ...,\n",
       "          1.10212984e-02, -3.99356661e-03, -8.38595256e-03],\n",
       "        ...,\n",
       "        [-5.64518236e-02, -2.87385564e-02,  2.88249291e-02, ...,\n",
       "         -3.03605571e-04, -4.53003356e-03,  8.53428058e-03],\n",
       "        [-1.39584085e-02,  1.94511004e-02, -5.07372841e-02, ...,\n",
       "         -3.45123261e-02, -3.79538387e-02,  2.45129932e-02],\n",
       "        [-3.43675986e-02, -5.85049205e-02, -2.15825308e-02, ...,\n",
       "          1.18607115e-02,  7.22798845e-03, -7.85856843e-02]],\n",
       "\n",
       "       [[-3.03771417e-03, -8.19578301e-03,  1.54584798e-03, ...,\n",
       "          6.85258943e-04, -1.03847422e-02, -5.12672309e-03],\n",
       "        [-4.64840187e-03, -3.39404284e-03, -9.35885310e-03, ...,\n",
       "          1.20137632e-02,  6.08581398e-03, -1.86477024e-02],\n",
       "        [-2.97541008e-03, -5.70372620e-04,  8.37891269e-03, ...,\n",
       "         -1.13861198e-02,  1.64196026e-02, -2.58545522e-02],\n",
       "        ...,\n",
       "        [-2.91791540e-02, -4.10315916e-02, -5.41261062e-02, ...,\n",
       "         -2.33263653e-02, -7.02480366e-03, -2.19675377e-02],\n",
       "        [ 1.50783639e-03, -4.97886389e-02, -8.59496444e-02, ...,\n",
       "         -1.93219557e-02, -2.26455685e-02,  2.00057812e-02],\n",
       "        [-1.07208207e-01, -3.62188779e-02, -7.14108273e-02, ...,\n",
       "          1.51249515e-02,  1.75146200e-02,  1.60892084e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset_train.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"text_generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"text_generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,626,025</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m6,400,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │     \u001b[38;5;34m1,311,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │    \u001b[38;5;34m25,626,025\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,436,201</span> (135.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m35,436,201\u001b[0m (135.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,436,201</span> (135.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m35,436,201\u001b[0m (135.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_527740/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 01:28:16.531165: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 01:28:16.531365: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:16.531474: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:16.531624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:16.531723: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 01:28:16.531794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /device:GPU:0 with 12702 MB memory:  -> device: 0, name: NVIDIA A16-16Q, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712971702.119369  527802 service.cc:145] XLA service 0x7f949c004850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1712971702.119408  527802 service.cc:153]   StreamExecutor device (0): NVIDIA A16-16Q, Compute Capability 8.6\n",
      "2024-04-13 01:28:22.170100: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-13 01:28:22.641004: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712971703.144974  527880 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_63', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712971703.484354  527882 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_63', 1748 bytes spill stores, 2168 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712971704.173049  527885 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_61', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712971706.135188  527880 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_61', 1740 bytes spill stores, 2152 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712971709.935412  527802 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1381s\u001b[0m 387ms/step - loss: 1.0624\n",
      "Epoch 2/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1374s\u001b[0m 388ms/step - loss: 0.7868\n",
      "Epoch 3/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1373s\u001b[0m 387ms/step - loss: 0.7487\n",
      "Epoch 4/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1372s\u001b[0m 387ms/step - loss: 0.7279\n",
      "Epoch 5/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1371s\u001b[0m 387ms/step - loss: 0.7142\n",
      "Epoch 6/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1370s\u001b[0m 387ms/step - loss: 0.7057\n",
      "Epoch 7/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1370s\u001b[0m 386ms/step - loss: 0.6999\n",
      "Epoch 8/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1369s\u001b[0m 386ms/step - loss: 0.6952\n",
      "Epoch 9/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1369s\u001b[0m 386ms/step - loss: 0.6933\n",
      "Epoch 10/10\n",
      "\u001b[1m3540/3540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1369s\u001b[0m 386ms/step - loss: 0.6914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f957d042e10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = loss, optimizer = optimizer)\n",
    "model.fit(dataset_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_eng.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_eng.tf/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('rnn_eng.keras')\n",
    "model.save('rnn_eng.h5')\n",
    "tf.saved_model.save(model, 'rnn_eng.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence = '<start>', max_len = 30):\n",
    "    #테스트를 위해 입력받은 init_sentence도 텐서 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype = tf.int64)\n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "    \n",
    "    #단어를 하나씩 예측해 문장 생성\n",
    "    while True:\n",
    "        #1. 입력받은 문장의 텐서 입력\n",
    "        predict = model(test_tensor)\n",
    "        #2. 예측된 값 중 가장 높은 확률인 word index를 출력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:, -1]\n",
    "        #3. 2에서 예측된 word index를 문장 뒤에 붙임\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis = 0)], axis = -1)\n",
    "        #4. 모델이 <end>를 예측했거나 max_len에 도달하면 문장 생성을 마침\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "        \n",
    "    generated = ''\n",
    "    #tokenizer를 활용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + ' '\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m in toronto <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = '<start> i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m in toronto <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = '<start> i', max_len = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = '<start> i love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is lovely, twisted <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = '<start> love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is lovely, twisted <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = '<start> Love is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love you everywhere, <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = '<start> love you ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> coming home <end> \n",
      "<start> everybody now, let me know <end> \n",
      "<start> steppin out <end> \n",
      "<start> ready to freefall <end> \n",
      "<start> hands on your body <end> \n",
      "<start> i m in toronto <end> \n",
      "<start> now i m on the outside <end> \n",
      "<start> see you in the rubble <end> \n",
      "<start> someone like you <end> \n",
      "<start> we re all in the same boat <end> \n"
     ]
    }
   ],
   "source": [
    "die_trying = ['coming', 'everybody', 'steppin', 'ready', 'hands', 'i', 'now', 'see', 'someone', 'we']\n",
    "for word in die_trying:\n",
    "    word = '<start> ' + word\n",
    "    print(generate_text(model, tokenizer, init_sentence = word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> take me home <end> \n",
      "<start> i m in toronto <end> \n",
      "<start> they don t know how to say <end> \n",
      "<start> so i m not going down <end> \n",
      "<start> there s no one <end> \n",
      "<start> right now right now <end> \n",
      "<start> and i m strikin <end> \n",
      "<start> no one s ever gotten <end> \n",
      "<start> seems like i m stoned <end> \n",
      "<start> still i m addicted to the <end> \n"
     ]
    }
   ],
   "source": [
    "take_me_home = ['take', 'i', 'they', 'so', 'there', 'right', 'and', 'no', 'seems', 'still']\n",
    "for word in take_me_home:\n",
    "    word = '<start> ' + word\n",
    "    print(generate_text(model, tokenizer, init_sentence = word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> they don t know how to say <end> \n",
      "<start> i m in toronto <end> \n",
      "<start> i just want to be with you <end> \n",
      "<start> what you shackled in <end> \n",
      "<start> that s the motto <end> \n",
      "<start> got a lot of shit <end> \n",
      "<start> but i m not dead <end> \n",
      "<start> you re the best of me <end> \n",
      "<start> took a sip and a goose <end> \n",
      "<start> bitch i m from zoo york <end> \n"
     ]
    }
   ],
   "source": [
    "imported_couches = ['they', 'i', 'i just', 'what', 'that', 'got', 'but', 'you', 'took', 'bitch']\n",
    "for word in imported_couches:\n",
    "    word = '<start> ' + word\n",
    "    print(generate_text(model, tokenizer, init_sentence = word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> how you move, i m not <end> \n",
      "<start> and i m strikin <end> \n",
      "<start> i m in toronto <end> \n",
      "<start> saying it s alright <end> \n",
      "<start> so i m not going down <end> \n",
      "<start> if you re resonating <end> \n",
      "<start> would you be my answer <end> \n",
      "<start> to the floor <end> \n",
      "<start> please don t leave me <end> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> you re the best of me <end> \n"
     ]
    }
   ],
   "source": [
    "better_man = ['how', 'and', 'i', 'saying', 'so', 'if', 'would', 'to', 'please', 'you']\n",
    "for word in better_man:\n",
    "    word = '<start> ' + word\n",
    "    print(generate_text(model, tokenizer, init_sentence = word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> you re the best of me <end> \n",
      "<start> god i hate <end> \n",
      "<start> covered in gold <end> \n",
      "<start> trusting you re sinking <end> \n",
      "<start> speak of the blue <end> \n",
      "<start> the way that i go <end> \n",
      "<start> night night <end> \n",
      "<start> lord i m gonna love you <end> \n",
      "<start> i m in toronto <end> \n",
      "<start> keep me satisfied <end> \n"
     ]
    }
   ],
   "source": [
    "god_of_my_dreams = ['you', 'god', 'covered', 'trusting', 'speak', 'the', 'night', 'lord', 'i', 'keep']\n",
    "for word in god_of_my_dreams:\n",
    "    word = '<start> ' + word\n",
    "    print(generate_text(model, tokenizer, init_sentence = word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> through the night <end> \n",
      "<start> far away <end> \n",
      "<start> no one s ever gotten <end> \n",
      "<start> we re all in the same boat <end> \n",
      "<start> shadows on the hills of the world <end> \n",
      "<start> where you belong <end> \n",
      "<start> i m in toronto <end> \n",
      "<start> in the morning <end> \n",
      "<start> <unk> <unk> <unk> <unk> <end> \n",
      "<start> the way that i go <end> \n"
     ]
    }
   ],
   "source": [
    "song_of_the_dusk = ['through', 'far', 'no', 'we', 'shadows', 'where', 'i', 'in', 'hearken', 'the']\n",
    "for word in song_of_the_dusk:\n",
    "    word = '<start> ' + word\n",
    "    print(generate_text(model, tokenizer, init_sentence = word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> no one s ever gotten <end> \n",
      "<start> can t trust you <end> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> everybody now, let me know <end> \n",
      "<start> you re the best of me <end> \n",
      "<start> father to the <unk> <end> \n",
      "<start> but i m not dead <end> \n",
      "<start> heart is like a dream <end> \n",
      "<start> don t you know <end> \n",
      "<start> there s no one <end> \n",
      "<start> your body is warm <end> \n"
     ]
    }
   ],
   "source": [
    "found_heaven = ['no', 'can', 'everybody', 'you', 'father', 'but', 'heart', 'don', 'there', 'your']\n",
    "for word in found_heaven:\n",
    "    word = '<start> ' + word\n",
    "    print(generate_text(model, tokenizer, init_sentence = word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_lyrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

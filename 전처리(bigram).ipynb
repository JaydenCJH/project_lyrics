{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import glob, os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인디\n"
     ]
    }
   ],
   "source": [
    "lyrics_data = 'songs_rawdata\\인디_5000곡_240402.csv'\n",
    "\n",
    "text_filepath = lyrics_data.split('\\\\')[-1].split('_')[0]\n",
    "\n",
    "print(text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 145 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lyrics_data = 'songs_rawdata\\인디_5000곡_240402.csv'\n",
    "\n",
    "text_filepath = lyrics_data.split('\\\\')[-1].split('.csv')[0] + '.txt'\n",
    "bigram_filepath = lyrics_data.split('\\\\')[-1].split('.csv')[0] + '_bigram.txt'\n",
    "bigram_model_filepath = lyrics_data.split('\\\\')[-1].split('_')[0] + '_bigram_model'\n",
    "corpus_filepath = lyrics_data.split('\\\\')[-1].split('.csv')[0] + '_bigram_corpus.csv'\n",
    "\n",
    "indi = pd.read_csv(lyrics_data)\n",
    "with open(text_filepath, 'w', encoding = 'utf-8') as f:\n",
    "    for lyrics in indi.Lyrics.values:\n",
    "        if pd.isnull(lyrics):\n",
    "            continue\n",
    "        f.write(lyrics+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_model_filepath = 'bigram_model'\n",
    "sentences_normalized_path = text_filepath\n",
    "unigram_sentences = LineSentence(sentences_normalized_path)\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.03 s\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigram_sentences_filepath = bigram_filepath\n",
    "with open(bigram_sentences_filepath, 'w', encoding = 'utf-8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = bigram_model[unigram_sentence]\n",
    "        f.write(' '.join(bigram_sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210235\n"
     ]
    }
   ],
   "source": [
    "txt_path = bigram_filepath\n",
    "raw_corpus = []\n",
    "\n",
    "with open(txt_path, 'r', encoding = 'utf-8') as f:\n",
    "    raw = f.read().splitlines()\n",
    "    raw_corpus.extend(raw)\n",
    "print(len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,¿,_]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r\"\\(.\\)\", \" \", sentence) # 7\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210226\n",
      "['<start> 돌아서는 너를 보며 <end>', '<start> 난 아무_말도 할_수 없었고 <end>', '<start> 슬퍼하기엔 짧았던 <end>', '<start> 나의 해는 저물어 갔네 <end>', '<start> 지나치는 모진 기억이 <end>', '<start> 바람_따라 흩어질 때면 <end>', '<start> 아무_일도 없듯이 보내주려 해 <end>', '<start> 아픈 맘이 남지 않도록 <end>', '<start> 안녕 멀어지는 나의 하루야 <end>', '<start> 빛나지 못한 나의 별들아 <end>']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if sentence[-1] == ':':\n",
    "        continue\n",
    "    if len(sentence)>150:\n",
    "        continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "print(len(corpus))\n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col':corpus})\n",
    "df.to_csv(corpus_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dan2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import glob, os, re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 84.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lyrics_data = 'songs_rawdata\\트로트_4990곡_240404.csv'\n",
    "\n",
    "text_filepath = lyrics_data.split('\\\\')[-1].split('.csv')[0] + '.txt'\n",
    "bigram_filepath = lyrics_data.split('\\\\')[-1].split('.csv')[0] + '_bigram.txt'\n",
    "bigram_model_filepath = lyrics_data.split('\\\\')[-1].split('_')[0] + '_bigram_model'\n",
    "corpus_filepath = lyrics_data.split('\\\\')[-1].split('.csv')[0] + '_bigram_corpus.csv'\n",
    "\n",
    "indi = pd.read_csv(lyrics_data)\n",
    "with open(text_filepath, 'w', encoding = 'utf-8') as f:\n",
    "    for lyrics in indi.Lyrics.values:\n",
    "        if pd.isnull(lyrics):\n",
    "            continue\n",
    "        f.write(lyrics+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_normalized_path = text_filepath\n",
    "unigram_sentences = LineSentence(sentences_normalized_path)\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.61 s\n",
      "Wall time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigram_sentences_filepath = bigram_filepath\n",
    "with open(bigram_sentences_filepath, 'w', encoding = 'utf-8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = bigram_model[unigram_sentence]\n",
    "        f.write(' '.join(bigram_sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201607\n"
     ]
    }
   ],
   "source": [
    "txt_path = bigram_filepath\n",
    "raw_corpus = []\n",
    "\n",
    "with open(txt_path, 'r', encoding = 'utf-8') as f:\n",
    "    raw = f.read().splitlines()\n",
    "    raw_corpus.extend(raw)\n",
    "print(len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,¿,_]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r\"\\(.\\)\", \" \", sentence) # 7\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201549\n",
      "['<start> I m hurting baby <end>', '<start> I m broken down <end>', '<start> I need your loving loving <end>', '<start> I need it now <end>', '<start> When I m without you <end>', '<start> I m something weak <end>', '<start> You got me begging begging <end>', '<start> I m on my knees <end>', '<start> I don t wanna be needing your love <end>', '<start> I just wanna be deep in your love <end>']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if sentence[-1] == ':':\n",
    "        continue\n",
    "    if len(sentence)>150:\n",
    "        continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "print(len(corpus))\n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col':corpus})\n",
    "df.to_csv(corpus_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dan2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

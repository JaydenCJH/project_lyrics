{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3769,"status":"ok","timestamp":1713140124436,"user":{"displayName":"최정민","userId":"13160064842180659050"},"user_tz":-540},"id":"kjIqJ0iOGJlJ"},"outputs":[],"source":["import re\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":172,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1138,"status":"ok","timestamp":1713149381572,"user":{"displayName":"최정민","userId":"13160064842180659050"},"user_tz":-540},"id":"PERE_i59GJlM","outputId":"07f81a73-92b3-4865-8b60-f13a869764e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["258604\n"]}],"source":["#txt_file = 'eng_elec_text.txt'\n","#txt_file = 'eng_elec_bigram.txt'\n","#txt_file = 'eng_folk_text.txt'\n","#txt_file = 'eng_folk_bigram.txt'\n","#txt_file = 'eng_hiphop_text.txt'\n","txt_file = 'eng_hiphop_bigram.txt'\n","#txt_file = 'eng_rnb_text.txt'\n","#txt_file = 'eng_rnb_bigram.txt'\n","#txt_file = 'eng_rock_text.txt'\n","#txt_file = 'eng_rock_bigram.txt'\n","#txt_file = 'kor_ballad_text.txt'\n","#txt_file = 'kor_ballad_bigram.txt'\n","#txt_file = 'kor_dance_text.txt'\n","#txt_file = 'kor_dance_bigram.txt'\n","#txt_file = 'kor_folk_text.txt'\n","#txt_file = 'kor_folk_bigram.txt'\n","#txt_file = 'kor_hiphop_text.txt'\n","#txt_file = 'kor_hiphop_bigram.txt'\n","#txt_file = 'kor_indie_text.txt'\n","#txt_file = 'kor_indie_bigram.txt'\n","#txt_file = 'kor_rnb_text.txt'\n","#txt_file = 'kor_rnb_bigram.txt'\n","#txt_file = 'kor_rock_text.txt'\n","#txt_file = 'kor_rock_bigram.txt'\n","#txt_file = 'kor_trot_text.txt'\n","#txt_file = 'kor_trot_bigram.txt'\n","raw_corpus = []\n","\n","with open(txt_file, 'r', encoding = 'utf-8') as f:\n","    raw = f.read().splitlines()\n","    raw_corpus.extend(raw)\n","print(len(raw_corpus))"]},{"cell_type":"code","execution_count":173,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10244,"status":"ok","timestamp":1713149391812,"user":{"displayName":"최정민","userId":"13160064842180659050"},"user_tz":-540},"id":"nU9YSp68GJlN","outputId":"66a3924e-d872-4121-8631-0554e3953ac1"},"outputs":[{"name":"stdout","output_type":"stream","text":["토크나이저:  <keras.src.preprocessing.text.Tokenizer object at 0x7f3ce0491f60> \n"," [[  2 119   6 ...   0   0   0]\n"," [  2 208 258 ...   0   0   0]\n"," [  2   4  41 ...   0   0   0]\n"," ...\n"," [  2   6  95 ...   0   0   0]\n"," [  2   8  16 ...   0   0   0]\n"," [  2   8  16 ...   0   0   0]]\n","Model: \"text_generator_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_11 (Embedding)    multiple                  3840256   \n","                                                                 \n"," lstm_23 (LSTM)              multiple                  5246976   \n","                                                                 \n"," lstm_24 (LSTM)              multiple                  8392704   \n","                                                                 \n"," lstm_25 (LSTM)              multiple                  8392704   \n","                                                                 \n"," dropout_1 (Dropout)         multiple                  0         \n","                                                                 \n"," dense_11 (Dense)            multiple                  15376025  \n","                                                                 \n","=================================================================\n","Total params: 41248665 (157.35 MB)\n","Trainable params: 41248665 (157.35 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Virtual devices cannot be modified after being initialized\n"]}],"source":["raw_corpus[:5]\n","\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip()                         #소문자 변경 후 양쪽 공백 제거\n","    sentence = re.sub(r\"([?.!,¿]).,\", r\" \\1 \", sentence)          #특수문자 양쪽에 공백 추가\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)                 #여러 개의 공백이 붙어있으면 하나의 공백으로\n","    sentence = re.sub(r\"[^a-zA-Z가-힣?!¿]+\", \" \", sentence)   #영어, 알파벳, ?, !, ¿ 제외 모두 공백으로\n","    sentence = sentence.strip()                                 #양쪽 공백 제거\n","    sentence = re.sub(r\"\\(.\\)\", \" \", sentence)                  #괄호 제거\n","    sentence = '<start> ' + sentence + ' <end>'                 #start, end 추가\n","    return sentence\n","\n","corpus = []\n","for sentence in raw_corpus:\n","    if len(sentence) == 0:\n","        continue\n","    if sentence[-1] == ':':\n","        continue\n","    if len(sentence)>150:\n","        continue\n","\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","\n","def tokenize(corpus):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 15000, filters = ' ', oov_token = '<unk>')\n","    tokenizer.fit_on_texts(corpus)\n","    tensor = tokenizer.texts_to_sequences(corpus)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n","    print('토크나이저: ', tokenizer, '\\n', tensor)\n","\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)\n","\n","src_input = tensor[:, :-1]\n","tgt_input = tensor[:, 1:]\n","\n","enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 1234)\n","\n","BUFFER_SIZE = len(src_input)\n","BATCH_SIZE = 256\n","steps_per_epochs = len(src_input) // BATCH_SIZE\n","VOCAB_SIZE = tokenizer.num_words + 1\n","\n","dataset_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n","dataset_train = dataset_train.shuffle(BUFFER_SIZE)\n","dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder = True)\n","\n","dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n","dataset_val = dataset_val.shuffle(BUFFER_SIZE)\n","dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder = True)\n","\n","\n","class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.lstm_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.lstm_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.lstm_3 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.dropout = tf.keras.layers.Dropout(0.2)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.lstm_1(out)\n","        out = self.lstm_2(out)\n","        out = self.lstm_3(out)\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","\n","embedding_size = 256\n","hidden_size = 1024\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n","model\n","\n","for src_sample, tgt_sample in dataset_train.take(1): break\n","model(src_sample)\n","\n","model.summary()\n","\n","tf.test.is_gpu_available()\n","\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_virtual_device_configuration(\n","            gpus[0],\n","            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]\n","        )\n","    except RuntimeError as e:\n","        print(e)"]},{"cell_type":"code","execution_count":174,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1713149391813,"user":{"displayName":"최정민","userId":"13160064842180659050"},"user_tz":-540},"id":"UCUGZlKQp7zy","outputId":"2176afda-8746-4ea4-fea8-bc502224ff42"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"text_generator_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_11 (Embedding)    multiple                  3840256   \n","                                                                 \n"," lstm_23 (LSTM)              multiple                  5246976   \n","                                                                 \n"," lstm_24 (LSTM)              multiple                  8392704   \n","                                                                 \n"," lstm_25 (LSTM)              multiple                  8392704   \n","                                                                 \n"," dropout_1 (Dropout)         multiple                  0         \n","                                                                 \n"," dense_11 (Dense)            multiple                  15376025  \n","                                                                 \n","=================================================================\n","Total params: 41248665 (157.35 MB)\n","Trainable params: 41248665 (157.35 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":175,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":675632,"status":"ok","timestamp":1713150629888,"user":{"displayName":"최정민","userId":"13160064842180659050"},"user_tz":-540},"id":"01TYrbHeGJlR","outputId":"c955508c-2aa1-4a0a-9da5-444bbeab4da9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","808/808 [==============================] - 126s 151ms/step - loss: 1.3481\n","Epoch 2/10\n","808/808 [==============================] - 123s 152ms/step - loss: 1.1411\n","Epoch 3/10\n","808/808 [==============================] - 123s 152ms/step - loss: 1.0826\n","Epoch 4/10\n","808/808 [==============================] - 124s 152ms/step - loss: 1.0398\n","Epoch 5/10\n","808/808 [==============================] - 123s 152ms/step - loss: 1.0048\n","Epoch 6/10\n","808/808 [==============================] - 124s 153ms/step - loss: 0.9715\n","Epoch 7/10\n","808/808 [==============================] - 124s 153ms/step - loss: 0.9418\n","Epoch 8/10\n","808/808 [==============================] - 124s 153ms/step - loss: 0.9142\n","Epoch 9/10\n","808/808 [==============================] - 124s 153ms/step - loss: 0.8888\n","Epoch 10/10\n","808/808 [==============================] - 124s 152ms/step - loss: 0.8652\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7f3d2c1e5720>"]},"execution_count":175,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(loss = loss, optimizer = optimizer)\n","model.fit(dataset_train, epochs = 10)"]},{"cell_type":"code","execution_count":176,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713150629889,"user":{"displayName":"최정민","userId":"13160064842180659050"},"user_tz":-540},"id":"m2YWdYLsGJlR"},"outputs":[],"source":["def generate_text(model, tokenizer, init_sentence = '<start>', max_len = 30):\n","    #테스트를 위해 입력받은 init_sentence도 텐서 변환\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype = tf.int64)\n","    end_token = tokenizer.word_index['<end>']\n","\n","    #단어를 하나씩 예측해 문장 생성\n","    while True:\n","        #1. 입력받은 문장의 텐서 입력\n","        predict = model(test_tensor)\n","        #2. 예측된 값 중 가장 높은 확률인 word index를 출력\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:, -1]\n","        #3. 2에서 예측된 word index를 문장 뒤에 붙임\n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis = 0)], axis = -1)\n","        #4. 모델이 <end>를 예측했거나 max_len에 도달하면 문장 생성을 마침\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = ''\n","    #tokenizer를 활용해 word index를 단어로 하나씩 변환\n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + ' '\n","\n","    return generated"]},{"cell_type":"code","execution_count":186,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":446,"status":"ok","timestamp":1713150758037,"user":{"displayName":"최정민","userId":"13160064842180659050"},"user_tz":-540},"id":"qr90TOC4JwgC","outputId":"a9d9a6f7-64d1-4f38-a487-258b71c83320"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> bitch i m a boss <end> '"]},"execution_count":186,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model, tokenizer, init_sentence = '<start> bitch')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTsIf2hvKV72"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
